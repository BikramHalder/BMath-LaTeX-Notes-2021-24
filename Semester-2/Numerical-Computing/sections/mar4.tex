\chapPreamble{18}{March 4}

\section{Fixed point methods}

Instead of trying to find a root to an equation of the form $f(x) = 0$, we can try to find a root to an equation of the form $x - f(x) = x$. Defining $g(x) \coloneqq x - f(x)$, we see that the task of finding a root of $f$ is equivalent to finding a \textbf{fixed point} of $g$ (a fixed point of a function $g$ is a real $\alpha$ such that $g(\alpha) = \alpha$).

We shall deal mainly with non-linear functions, as finding the roots/fixed points of linear functions can be done by employing the techniques of linear algebra, which we have already covered.

Therefore, we now set for ourselves the task of finding fixed points $\alpha$ of a function $g$. But first, we recall some theorems that we proved in previous lectures

\begin{thm}
	\label{mar4:thm:efp}
	\hfill

	If the following conditions obtain,
	\begin{itemize}
		\item
		      $g \colon [a, b] \to [a, b]$.

		\item
		      $g$ is continuous on $[a, b]$.

	\end{itemize}
	Then $g$ has a (not necessarily unique) fixed point in $[a, b]$.
	\hfill\qed
\end{thm}

\begin{thm}
	\label{mar4:thm:ufp}
	\hfill

	If the following conditions obtain,
	\begin{itemize}
		\item
		      $g \colon [a, b] \to [a, b]$.

		\item
		      $g$ is continuous on $[a, b]$.

		\item
		      $g'$ exists in $[a, b]$.

		\item
		      $g'$ is continuous on $[a, b]$.

		\item
		      $\lambda \coloneqq \max_{x \in [a, b]} \abs{g'(x)} < 1$.

	\end{itemize}
	Then $g$ has a \textbf{unique} fixed point in $[a, b]$.
	\hfill\qed
\end{thm}
Note that we have the same problem here as when we did the bisection method; the problem of finding a suitable interval ($[a, b]$ in this case). But let us assume that you have found such an interval, and proceed.

Under the assumptions of theorem \ref{mar4:thm:ufp}, if we start with an initial guess $x_0 \in [a, b]$, and define subsequent guesses using the recursion
\begin{defn}
	\label{mar4:def:iter}
	\[
		x_{n+1} = g(x_n) \qquad \forall(n \geq 0)
	\]
\end{defn}
then if we denote the unique fixed point of $g$ in $[a, b]$ by $\alpha$, we have
\begin{thm}
	\label{mar4:thm:2c}
	\[
		\abs{\alpha - x_n} \leq \lambda^n\abs{\alpha - x_0}
	\]
\end{thm}

\begin{proof}
	Assuming the preconditions of theorem \ref{mar4:thm:ufp}, $g \colon [a, b] \to [a, b]$. Combining that with the fact that $x_0 \in [a, b]$, it is easy to see by induction that $x_n \in [a, b]$ for all $n \geq 0$.

	Now, by definition \ref{mar4:def:iter}, and the fact that $g(\alpha) = \alpha$, we have
	\[
		\alpha - x_{n+1} = g(\alpha) - g(x_n) = g'(c_n)(\alpha - x_n)
	\]
	for some $c_n \in (\alpha, x_n)$, by the \emph{mean value theorem}. Since by the preconditions of theorem \ref{mar4:thm:ufp} $\lambda \coloneqq \max_{x \in [a, b]} \abs{g'(x)} < 1$, we obtain from the above equation
	\[
		\abs{\alpha - x_{n+1}} \leq \lambda\abs{\alpha - x_n}
	\]
	By induction, we get
	\[
		\abs{\alpha - x_n} \leq \lambda^n\abs{\alpha - x_0}
	\]
	\hfill
\end{proof}
\begin{corr}
	Under the assumptions of theorem \ref{mar4:thm:ufp},
	\[
		\lim_{n\to\infty} x_n = \alpha
	\]
\end{corr}
\begin{proof}
	By theorem \ref{mar4:thm:2c}, $\abs{\alpha - x_n} \leq \lambda^n\abs{\alpha - x_0}$. Since $\lambda < 1$, we get
	\[
		\lim_{n\to\infty}\abs{\alpha - x_n} = 0
	\]
	using the squeeze theorem (the L.H.S. of the squeeze is $0 \leq \abs{\alpha - x_n}$, which follows from properties of the $\abs{\cdot}$ function).
	\hfill
\end{proof}
\begin{corr}
	Under the assumptions of theorem \ref{mar4:thm:ufp},
	\[
		\abs{\alpha - x_n} \leq \frac{\lambda^n}{1 - \lambda}\abs{x_0 - x_1}
	\]
\end{corr}
\begin{proof}
	Note that
	\begin{align*}
		                  & \abs{\alpha - x_0}
		\lao[\leq]{1} \abs{\alpha - x_1} + \abs{x_0 - x_1}
		\lao[\leq]{2} \lambda\abs{\alpha - x_0} + \abs{x_0 - x_1} \\\\
		\implies          &
		(1-\lambda)\abs{\alpha - x_0}
		\leq
		\abs{x_0 - x_1}                                           \\\\
		\lao[\implies]{3} &
		\abs{\alpha - x_0}
		\leq
		\frac{\abs{x_0 - x_1}}{1-\lambda} \tag{4}
	\end{align*}
	where inequality (1) is the triangle inequality, and inequality 2 follows from theorem \ref{mar4:thm:2c}. Implication (3) is justified as $\lambda < 1$ and therefore we are not dividing by 0. But then
	\[
		\abs{\alpha - x_n}
		\lao[\leq]{5}
		\lambda^n\abs{\alpha - x_0}
		\lao[\leq]{6}
		\frac{\lambda^n}{1-\lambda}\abs{x_0 - x_1}
	\]
	where inequality (5) follows from theorem \ref{mar4:thm:2c}, and inequality (6) follows from inequality (4) above.

	\hfill
\end{proof}
Now we come to what is probably the most important theorem in this lecture.
\begin{thm}
	Under the assumptions of theorem \ref{mar4:thm:ufp},
	\label{mar4:thm:bigtp}
	\[
		\lim_{n\to\infty} \frac{\alpha - x_{n+1}}{\alpha - x_n} = g'(\alpha)
	\]
\end{thm}
\begin{proof}
	Note that by the mean value theorem, for all $n \geq 0$ we have $\alpha - x_{n+1} = g(\alpha) - g(x_n) = g'(c_n)(\alpha - x_n)$ for some $c_n \in (\alpha, x_n)$. Therefore,
	\[
		\lim_{n\to\infty} \frac{\alpha - x_{n+1}}{\alpha - x_n} = \lim_{n\to\infty} g'(c_n)
	\]
	But since $x_n \to \alpha$, $c_n \to \alpha$ too, and then using the fact that $g'$ is continuous in $[a, b]$, we get
	\[
		\lim_{n\to\infty} g'(c_n) = g'(\alpha)
	\]
	\hfill
\end{proof}

\begin{defn}
	Suppose we have a sequence $\{y_n \}$ that converges to $\beta$. We say that $\{y_n \}$ converges to $\beta$ \textbf{linearly}, if for all $n$
	\[
		\beta - y_{n+1} \approx c(\beta - y_n)^p
	\]
	with $p = 1$.

	If $p > 1$, we say that the sequence $\{y_n \}$ converges to $\beta$ \textbf{super-linearly}.
\end{defn}
We are now ready to state the final theorem of this lecture.
\begin{thm}
	If we assume the preconditions of \ref{mar4:thm:ufp}, and we additionally assume that $g'(\alpha) \neq 0$, $x_n$ converges to $\alpha$ linearly.
\end{thm}
\begin{proof}
	Theorem \ref{mar4:thm:bigtp} tells us that $\lim_{n\to\infty} \frac{\alpha - x_{n+1}}{\alpha - x_n} = g'(\alpha)$. That implies, as long as $g'(\alpha) \neq 0$ and therefore higher order terms do not dominate, that for large $n$,
	\[
		\alpha - x_{n+1} \approx g'(\alpha)(\alpha - x_n)
	\]
	(The convergence is guaranteed both by theorem \ref{mar4:thm:efp} and by the fact that $g'(\alpha) < 1$, which follows from the preconditions of \ref{mar4:thm:ufp}).
	\hfill
\end{proof}

We end by remarking that fixed point iterations are very easy to program owing to the simple nature of their iterations.

% [Stopped at 37:36 $\pm$ a few seconds. The remainder of the lecture is about polynomial interpolation.]
\newpage
\section{Introduction to Interpolation}
\subsection{What is Interpolation?}
Let $f(x)$ be a function on an interval $[a, b]$. The goal is to construct a function $g(x)$ that
approximates $f$ to within a given error. To do so, we must specify:
\begin{itemize}
	\item The structure of the approximating functions
	\item The way of measuring error
\end{itemize}
And, We have 2 problems now:

\begin{itemize}
	% \begin{problem}
	\item \hypertarget{mar4:int_prob:1}{\textbf{Problem I:}} The function $f(x)$ is given (like $f(x) = e^x$), can be evaluated at any point	$x$, and we seek a simple function $g(x)$ (like $g(x) = ax + b$) that best approximates $f$. The \textit{simple} part constrains the possible accuracy.
	      % \end{problem}
	      % \begin{problem}
	\item \hypertarget{mar4:int_prob:2}{\textbf{Problem II:}} Values are given at a set of points: $(x_0, f_0), \ldots ,(x_n, f_n) $ with $f_j = f(x_j)$ but $f(x)$ is not known. The amount and type of data constrains the possible accuracy and what approximations may be constructed.
	      % \end{problem}
\end{itemize}

Our approach to approximation is known as \textbf{interpolation}. The process of finding the value of $f(x)$ corresponding to any value of $x = x_i$ between $x_0$ and $x_n$ is called interpolation.
\begin{defn}[\textbf{Interpolation}]
	The technique of estimating the value of a function for any intermediate value of the independent variable.
\end{defn}
If the function $f$ is known explicitly, then the value of $f(x)$ corresponding to any value of $x$ can easily be found. Conversely, if the form of $f(x)$ is not known (as is the case in most of the applications), it is very difficult to determine the exact form of $f(x)$ with the help of tabulated set of values $(x_i, f_i)$.

\begin{figure}[H]
	\centering
	\def\svgwidth{0.8\textwidth}
	\import{./figures}{interpol_intro.pdf_tex}
	\caption{}
	\label{fig1:mar4}
\end{figure}

\textit{In such cases}, $f(x)$ is replaced by a simpler function $\phi(x)$ which assumes the same values as those of $f(x)$ at the tabulated set of points. Any other value may be calculated from $\phi(x)$ which is known as the \textbf{interpolating function} or \textbf{smoothing function}.

\begin{rmk}
	If $\phi(x)$ is a polynomial, then it called the \textbf{interpolating polynomial} and the process is called the \textbf{polynomial interpolation}. Similarly, when $\phi(x)$ is a finite trigonometric series, we have \textbf{trigonometric interpolation}. But we shall confine ourselves to polynomial interpolation only.
\end{rmk}

\begin{tcolorbox}[
		colback = red!5,
		colframe = red!75
	]
	\textbf{Caution (interpolation vs. approximation)}: Note that \textit{interpolation} is not exactly the same as \textit{approximation} - it is a strategy that one hopes will approximate the function. There is no exact and unique solution. The actual function is NOT known and CANNOT be determined from the tabular data. In the case of \hyperlink{mar4:int_prob:2}{Problem II} where data is given, interpolation is natural since it uses precisely
	the data we are given.\\
	For \hyperlink{mar4:int_prob:1}{Problem I} (where $f$ is given), it is not obvious that interpolation is the right way to obtain a small max-norm (or any other error). Compare, for instance, to a best-fit line that does not have to pass through any points.
\end{tcolorbox}

From the set $\{ (x_0, f_0), \ldots ,(x_n, f_n) \}$ with $f_j = f(x_j)$, we would like to write,
\[
	f(x) \approx \phi(x) \text{ such that } \phi(x_j) = f_j \ \forall j \in \{0,1, \ldots , n\}
\]

\begin{example}
	Some examples of various kind of interpolation are following:
	\begin{itemize}
		\item Linear Interpolation (\textit{General construction idea})
		      \\ Here, we choose a set of $(n+1)$ functions $ \{\phi_i\}_{i=0}^n $ and assume that $\phi$ is linearly dependent on $\phi_i$'s.
		      \[
			      \phi(x) = \sum_{i=0}^{n}a_i\phi_i(x)
		      \]
		      \begin{itemize}
			      \item[(a)] Polynomial Interpolation ($ \phi_i(x) = x^i $)
				      \[
					      \Phi(x) = \sum_{i=0}^{n}a_ix^i
				      \]
			      \item[(b)] \[\phi(x) = \sum_{r=0}^{n}a_re^{\iota rx} = \sum_{r=0}^{n}a_r\cos(rx) + \iota\sum_{r=0}^{n}a_r\sin(rx) \]
			      \item[(c)] Cubic Spline interpolation
				      \begin{itemize}
					      \item Here, we assume that $ \phi(x) $ coincides with a cubic polynomial on every interval $ [x_i,x_{i+1}], i = 0,1,\ldots,n $
					      \item Polynomials on different sub-intervals need not match.
				      \end{itemize}
		      \end{itemize}
		\item Non-Linear Interpolation
		      \begin{itemize}
			      \item Rational function interpolation
			            \[
				            \phi(x) = \frac{a_0+a_1x+\cdots+a_nx^n}{b_0+b_1x+\cdots+b_mx^m}
			            \]
			            Note that, total $ n+m+2 $ parameters $ a_0,a_1,\ldots,a_n,b_0,b_1,\ldots,b_n $ need to be determined here.
		      \end{itemize}
	\end{itemize}

\end{example}

\begin{rmk}
	Interpolation is a "guess" of function in a "valid domain" given to us.
	\[ \text{In the above case, the "valid domain" is  } [\min_j x_j, \max_j x_j] \]
	On the other hand "guessing" a function outside the domain is known as \textbf{Extrapolation}.
\end{rmk}






%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../NC"
%%% End:
