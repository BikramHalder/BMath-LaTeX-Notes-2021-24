\chapter*{Lecture 16 \\ February 28, Part A}
\addcontentsline{toc}{chapter}{Lecture 16 (February 28, Part A)}
\setcounter{chapter}{16}
\setcounter{section}{0}

\section{Error Analysis of the Newton Raphson method}

We begin by assuming that we are to use the Newton Raphson method to find a root $\alpha$ of a function $f$ (\textbf{which we have a closed form expression for}). We start with an initial guess of $\alpha$, which we denote by $x_0$.
The iteration used in the Newton Raphson method is
\begin{defn}
  \label{feb28:def:iter}
  \[
    x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} \qquad \forall(n \geq 0)
  \]
\end{defn}
We can calculate $f(x_n)$ and $f'(x_n)$ because we have a closed form expression for $f$.

Note that if any of the $f'(x_n)$'s are 0, the iteration immediately fails at that stage. This is a flaw present in the Newton Raphson method that the bisection method doesn't suffer from.

In order to carry out the error analysis, we have to make some assumptions which we list below.
\begin{itemize}
\item
  There exists a $\rho > 0$ such that $f$ is continuously differentiable atleast twice in $[\alpha - \rho, \alpha + \rho]$. This assumption must be taken on faith.
\item
  $f'(\alpha) \neq 0$. If this assumption is not true, it is not possible for the iteration to converge to $\alpha$, as the term $\frac{f(x_n)}{f'(x_n)}$ would blow up to infinity if it did.
\end{itemize}

Note that the two assumptions listed above imply that $f' \neq 0$ in some neighbourhood of $\alpha$.

Next, we assume that for some $n$ $x_n$ is sufficiently close to $\alpha$. Sufficiently close as in sufficiently close for the manipulations that follow. Then, using a Taylor Expansion (we can do that because we have a closed form expression for $f$), and cutting it off after 3 terms, we can write
\begin{alignat*}{3}
  &&f(\alpha)
  &=
    f(x_n) + (\alpha - x_n)f'(x_n) + \frac 12 (\alpha - x_n)^2f''(x_n) \\
  \implies&&
  0
  &\lao{1}
  f(x_n) + (\alpha - x_n)f'(x_n) + \frac 12 (\alpha - x_n)^2f''(x_n) \\
  \implies&&
  0
  &\lao{2}
  \frac{f(x_n)}{f'(x_n)} + (\alpha - x_n) + (\alpha - x_n)^2\frac{f''(x_n)}{2f'(x_n)} \\
\end{alignat*}
where equality (1) follows from the fact that $\alpha$ is a root of $f$, equality (2) is obtained by dividing both sides of the equation by $f'(x_n)$, which we can do since $f'(x_n)$ is close to $f'(\alpha) \neq 0$, which in turns hold because $f'$ is continuous (by assumption) and $x_n$ is sufficiently close to $\alpha$. Using equation \ref{feb28:def:iter}, we obtain
\begin{corr}
  \label{feb28:corr:noncomp}
  \[
    \alpha - x_{n+1} = (\alpha - x_n)^2\left[-\frac{f''(x_n)}{2f'(x_n)}\right]
  \]
\end{corr}
Therefore we may say that
\[
  \text{Error in $x_{n+1}$}
  \sim
  \left(\text{Error in $x_{n}$}\right)^2
\]
where $\sim$ stands for proportionality. (Note that we are assuming that $f'(x_n) \neq 0$). For convergence, $\frac{f''(x_n)}{2f'(x_n)}$ must not be too big.

Next, we run into yet another problem: We cannot compute either side of equation \ref{feb28:corr:noncomp}, as we don't know the value of $\alpha$. Which brings us to the next section.

\section{Bounding some Parameters}

Although we don't know the value of $\alpha$, we are working with an $x_n$ which is ``sufficiently close'' to it. In that case, since $f''$ and $f'$ are continuous at $\alpha \in [\alpha - \rho, \alpha + \rho]$, we have
\[
  M \coloneqq -\frac{f''(\alpha)}{2f'(\alpha)} \approx -\frac{f''(x_n)}{2f'(x_n)}
\]
Rewriting \ref{feb28:corr:noncomp}, we get
\begin{alignat*}{3}
  &&\alpha - x_{n+1}
  &\approx
  (\alpha - x_n)^2M \\
  \implies&& M\left(\alpha - x_{n+1}\right)
  &\approx
  \left[M\left(\alpha - x_{n}\right)\right]^2 \\
\end{alignat*}
Now, we carried out this analysis assuming that the iteration converges. If we also assume that iterates do not again stray far from $\alpha$ after they get close to it (this is possible because for convergence only the behaviour in the long run matters), and if we assume that $x_0$ is sufficiently close to $\alpha$ (previously we'd let $x_n$ be sufficiently close to $\alpha$ for some $n$ whose value was unknown to us), using induction we get
\begin{thm}
  \label{feb28:thm:exp}
  \[
    M\left(\alpha - x_{n}\right)
    \approx
    \left[M\left(\alpha - x_{0}\right)\right]^{2^n}
  \]
\end{thm}
Since the iteration converges, it cannot be the case that $\abs{M(\alpha - x_0)} \geq 1$, which implies that
\begin{thm}
  Under the conditions required for \ref{feb28:thm:exp} to hold,
  \[
    \abs{M(\alpha - x_0)} < 1 \implies \abs{\alpha - x_0} < \frac{1}{\abs{M}}
  \]
\end{thm}
The above theorem implies, among other things, that if $\abs{M}$ is very large, our initial guess $x_0$ will have to be very small, for the error analysis in theorem \ref{feb28:thm:exp} to hold. Intuitively, $f$ should not be ``flat'' around the root. Newton Raphson will fail often and fail miserably if $f''(\alpha)$ is finite, and $f'(\alpha) = 0$, which happens even when you're dealing with relatively ordinary functions like the trigonometric functions.

The case where $f''(\alpha)$ blows up to infinity doesn't happen as often; for examples look at functions that have exponential-like growth.

\section{Even more computational problems}

We once again ask ourselves - what are the things we can \textit{compute}? A little reflection should reveal to you that we can compute the following and just that:
\begin{itemize}
\item
  We have a closed form expression for $f$, and hence

\item
  We know the values of $x_0, x_1, \dots$.

\item
  We know the values of $f(x_0), f(x_1), \dots$.

\item
  We know the values of $f'(x_0), f'(x_1), \dots$.

\item
  We know the values of $f''(x_0), f''(x_1), \dots$.
\end{itemize}
Now, using the mean value theorem and the fact that $f(\alpha) = 0$, if $x_n$ is sufficiently close to $\alpha$, we have
\[
  f(x_n) = f(x_n) - f(\alpha) = f'(\xi_n)(x_n - \alpha)
\]
for some $\xi_n \in (\alpha, x_n)$. The last equation can be written as
\[
  \alpha - x_n = - \frac{f(x_n)}{f'(\xi_n)}
\]
If $- \frac{f(x_n)}{f'(\xi_n)} \approx - \frac{f(x_n)}{f'(x_n)}$, then from definition \ref{feb28:def:iter}, we obtain
\[
  \alpha - x_n \approx x_{n+1} - x_n
\]
One way to use this is to find such a $\xi_n$, in which case you can guess where $\alpha$ lies relative to $x_n$, use the above equation for error analysis, etc.

We end by noting that although the bisection method always works (provided you can bracket the roots), but Newton Raphson can fail. However when it does work, Newton Raphson converges to the root much faster than the bisection method.

\section{The Secant method}

We wish to find a root $\alpha$ of a function $f$. Start with \textbf{\textit{two}} initial guesses of $\alpha$ namely $x_0$ and $x_1$. Ideally, the guesses should bracket $\alpha$. Then, compute subsequent guesses using the following iteration
\begin{defn}
  \[
    x_{n+1} = x_n - f(x_n)\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}
  \]
\end{defn}
Note the similarity between the Newton Raphson method and the Secant method! (The derivative in the Newton Raphson method is replaced with a discrete version of it)

Unfortunately, the Secant method converges more slowly than the Newton Raphson method, and thus finds little practical use.

It is possible for the guesses to alternate between a few values, and not converge.

Also, just because the initial guesses bracket the root, that doesn't mean the subsequent guesses will too.

Normally, when we compute $x_{n+1}$, we discard $x_{n-1}$. But you can instead discard $x_n$ if you need to, with the goal of having guesses that bracket the root. Although even an uninterrupted bracket fails when the expressions of the form $f(x_n) - f(x_{n-1})$ are too large, and thus the brackets keep getting larger and larger instead of smaller.

For an example of what can go wrong with the secant method consider the case of a vertical parabola with vertex at 0, and initial guesses $\pm 1$.

It is not clear to me whether the secant method will converge if our initial guesses bracket the root, we always choose our subsequent guesses so that they always bracket the root, and $f$ has different signs at our guesses (including the initial guesses). Someone should probably investigate.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../NC"
%%% End:
