\chapter*{Lecture 8, Febuary 14 \\ {\Large (Valentine's Special) }}
\addcontentsline{toc}{chapter}{Lecture 8, Febuary 14 (Valentine's Special)}
\setcounter{chapter}{8}
\setcounter{section}{0}

\section{Fundamental Theorem of Calculus}

\begin{defn}\label{def1:feb14}
    Let $S \subseteq \bb{R}$, and let $f : S \to \bb{R}$ be a function. A differentiable function $F$ is called \texty{antiderivative} or \texty{primitive} of $f$ on $S$, if 
    \[
        f(x) = F'(x), \ \forall \, x \in S   
    \]  
\end{defn}

\begin{example}\label{eg1:feb14}
    Let $f : [0,1] \to \bb{R}$ be defined by $f(x) = x, \ \forall \, x \in [0,1]$ then $F(x) = \frac{1}{2}x^2 + c$ is a antiderivative of $f$ on $[0,1]$, where $c$ is a fixed real number. Thus clearly antiderivatives are not unique, since if $F$ is an antiderivative of $f$ on some set $S$, then so is $F' := F+c$, where $c$ is any real number.
\end{example}

The natural question that arises now is do all functions have antiderivatives? 

\begin{example}\label{eg2:feb14}
    Consider $f : [-1.1] \to \bb{R}$ as follows 
    \[ 
        f(x) = 
        \begin{cases}
            1 & \mbox{ if } x \in [0,1] \\ 
            0 & \mbox{ otherwise }    
        \end{cases}    
    \]
    Then $f$ doesn't have any antiderivative. To see this observe if $f$ indeed had an antiderivative $F$, then we would have 
    \[
        F'(x) = f(x) = 
        \begin{cases}
            1 & \mbox{ if } x \in [0,1] \\ 
            0 & \mbox{ otherwise }    
        \end{cases}      
    \]
    But then by \texty{Darboux's theorem}\footnote{\texty{Darboux's theorem:} Let $f : [a,b] \to \bb{R}$ be a differentiable function such that $f'(a) \neq f'(b)$, then for any $c$ in between $f'(a)$ and $f'(b)$, there exists $\eta \in (a,b)$ such that $f'(\eta) = c$.}, we would get that $\forall \, c \in (0,1)$, there exists a $\eta \in [-1,1]$ such that $f(\eta) = F'(\eta) = c$ (Contradiction! since range of $f$ is simply $\{0,1\}$).
\end{example}

Thus, its evident from \texty{example} $\ref{eg2:feb14}$, that there are functions which do not have any antiderivatives. Later we will see that for any continuous function there always exists an antiderivative, this will follow as a consequence of \texty{Second Fundamental Theorem of Calculus}. But first we look at the \texty{First Fundamental Theorem of Calculus}.

\subsection{First Fundamental Theorem of Calculus}

\begin{thm}\label{thm1:feb14}
    Let $f \in \mR[a,b]$, and suppose $F$ is an antiderivative of $f$ on $(a,b)$, then 
    \[
        \int_a^b f = F(b) - F(a)  
    \] 
\end{thm}
\begin{prf}
    Let $P \in \mP[a,b]$, suppose $ P = \{a=x_0, x_1, \dots, x_{n-1},x_n =b\}$ where $x_0 < x_1 < \cdots < x_n $. Then observe that we can write 
    \[ 
        F(b) - F(a) = \sum_{j=1}^n (F(x_j) - F(x_{j-1}))    
    \] 
    But then since $F$ is antiderivative of $f$ on $(a,b)$, we have $F$ is differentiable on $(a,b)$, and hence using \texty{Mean Value Theorem} we get that 
    \begin{align*}
        F(x_j) - F(x_{j-1}) &= F'(\zeta_j) (x_j - x_{j-1}) \\ 
                            &= f'(\zeta_j) \Delta x_j
    \end{align*}
    for some $\zeta_j \in (x_{j-1},x_j)$. Hence, we get that 
    \[
        L(f,P) \leq \sum_{j=1}^n f(\zeta_j) \Delta x_j = \sum_{j=1}^n (F(x_j) - F(x_{j-1})) = \sum_{j=1}^n f(\zeta_j) \Delta x_j \leq U(f,P)  
    \]
    and thus, we have shown that 
    \begin{align}\label{eq1:feb14}
        L(f,P) \leq F(b) - F(a) \leq U(f,P) &&\forall \, P \in \mP[a,b] 
    \end{align}
    but then from properties of supremum and infimum of a set we deduce that 
    \[
        \underline{\int_a^b} f \leq F(b) - F(a) \leq \overline{\int_a^b} f   
    \]
    But since $f \in \mR[a,b]$, we get that $\underline{\int_a^b}f = \int_a^b f = \overline{\int_a^b} f$, and thus we have proved that 
    \[
        \int_a^b f = F(b) - F(a)  
    \]
\end{prf}

Thus, \texty{theorem} $\ref{thm1:feb14}$ tells us that, if we already know the antiderivative of the function $f$, we can easily compute its integral.

\subsection{Second Fundamental Theorem of Calculus}

\begin{thm}\label{thm2:feb14}
    Let $f \in \mR[a,b]$, and define the function $F : [a,b] \to \bb{R}$, as 
    \begin{equation}\label{eq2:feb14}
        F(x) := \int_a^x f(t) \dd{t}, \ \forall \, x \in [a,b]
    \end{equation}
    then the following statements are true:
    \begin{enumerate}
        \item[(i)] $F \in \mathcal{C}[a,b]$.
        \item[(ii)] If $f$ is continuous at $x_0 \in (a,b)$, then $F$ is differentiable at $x_0$, and in particular we have $F'(x_0) = f(x_0)$.
        \item[(iii)] If $f$, is continuous from right at $a$, then $F$ has right-hand derivative at $a$, and we have $F'_{+}(a) = f(a)$. Similarly, if $f$ is continuous from left at $b$, then $F$ has left-hand derivative at $b$, and we have $F'_{-}(b) = f(b)$. 
    \end{enumerate}
\end{thm}

\begin{prf}
\begin{enumerate}
    \item[(i)] Let $M = \sup_{x \in [a,b]} |f(x)|$, let $x,y \in [a,b]$, then we have 
    \begin{align*}
        |F(x) - F(y)| = \left| \int_y^x f(t) \dd{t} \right| 
    \end{align*}
    Now note that from $-M \leq f(t) \leq M $ we have 
    \begin{align*} 
        &\Rightarrow -\int_y^x M \dd{t} \leq \int_y^x f(t) \dd{t} \leq \int_x^y M \dd{t} \\ 
        &\Rightarrow -M(x-y) \leq \int_y^x f(t) \dd{t} \leq M(x-y) \\ 
        &\Rightarrow \left| \int_y^x f(t) \dd{t} \right| \leq M |x-y|
    \end{align*}
    and hence we get that 
    \[
        |F(x) - F(y)| \leq M |x-y|  
    \]
    and hence, $F$ from here we can easily see that $F \in \mathcal{C}[a,b]$, and we further conclude that $F$ is in fact \texty{Lipschitz continuous}.

    \item[(ii)] Observe that we can write 
    \[
        \frac{F(x)-F(x_0)}{x-x_0} - f(x_0) = \frac{1}{x-x_0} \int_{x_0}^x \left(f(t) - f(x_0)\right) \dd{t}   
    \] 
    But then since $f$ is continuous at $x_0$, we have for all $\eps > 0$, there exists a $\delta > 0$, such that 
    \[
        |f(t) - f(x_0)| < \eps, \ \forall \, t \in (x_0 - \delta, x_0 + \delta)  
    \]
    and hence for $x \in (x_0-\delta, x_0-\delta)$ we have 
    \begin{align*}
        \left| \frac{F(x)-F(x_0)}{x-x_0} - f(x_0) \right| &= \frac{1}{|x-x_0|} \left| \int_{x_0}^x (f(t) - f(x_0)) \dd{t} \right| \\ 
        &\leq \frac{1}{|x-x_0|} \int_{x_0}^x |f(t) - f(x_0)| \dd{t} \\ 
        &\overset{(1)}{<} \frac{1}{|x-x_0|} \int_{x_0}^x \eps \dd{t} \\ 
        &\leq \frac{1}{|x-x_0|} \cdot |x-x_0| \cdot \eps = \eps
    \end{align*}
    where $(1)$ is true because for $x \in (x_0 - \delta, x_0+\delta)$, we have $t \in (x_0-\delta,x_0+\delta)$. Hence, if $x_0$ is a point continuity of $f$, we have $F$ is differentiable at $x_0$, and further we have $F'(x_0) = f(x_0)$.

    \item[(iii)] Now if $f$ is continuous from right at $a$, then similar arguments as in part (ii), will work, just that we in this case we have to work on an interval $(a,a+\delta)$, for some $\delta > 0$. The proof for $f$ is continuous from left at $b$, is also similar (in this case we will have to work on an interval $(b-\delta,b)$ for some $\delta > 0$).  
\end{enumerate} 
\end{prf} 

\begin{corr}\label{cor1:feb14}
    Let $f \in \mathcal{C}[a,b]$, then 
    \[
        \dv{x} \left( \int_a^x f(t)dt \right) = f(x), \ \forall \, x \in [a,b]    
    \]
\end{corr}
\begin{prf}
    Directly follows from \texty{theorem} $\ref{thm2:feb14}$.
\end{prf}

\begin{example}\label{eg3:feb14}
    Consider $f : [0,2] \to \bb{R}$, defined by 
    \[
        f(x) = \begin{cases}
            1 & \mbox{ if } x \in [0,1] \\ 
            0 & \mbox{ otherwise}
        \end{cases}  
    \]
    Then it is obvious that $f \in \mR[0,2]$. Now the function $F(x) := \int_0^x f(t) \dd{t}$, where $x \in [0,2]$ can be easily computed, and we get 
    \[
        F(x) = \begin{cases}
            x & \mbox{ if } 0 \leq x \leq 1 \\ 
            1 & \mbox{ if } 1 < x \leq 2
        \end{cases}  
    \]
    But then we easily observe that $F \in \mathcal{C}[0,2]$, but evidently $F$ is not differentiable at $1$, which is in fact the point of discontinuity of $f$.
\end{example}

\section{Integration By Parts}

\begin{thm}\label{thm3:feb14} 
    Let $f, g \in \mathcal{D}[a,b]$, and further assume that $f', g' \in \mR[a,b]$, then 
    \begin{equation}\label{ibp}
        \int_a^b f(x)g'(x) \dd{x} + \int_a^b f'(x)g(x) \dd{x} = f(b)g(b) - f(a)g(a)
    \end{equation}
\end{thm}

\begin{prf}
    Set $u = fg$, then since $f,g \in \mathcal{D}[a,b]$, we have $g \in \mathcal{D}[a,b]$, and in particular we have $u' = fg'+f'g$, then from \texty{first fundamental theorem of calculus} $\ref{thm1:feb14}$, we get that 
    \[
         \int_a^b u'(x) \dd{x} = u(b) - u(a)   
    \]
    but then observe that from \texty{linearity of integral} (\texty{theorem} \ref{thm3:feb4} (i)) we get that 
    \[
        \int_a^b f(x)g'(x) \dd{x} + \int_a^b f'(x)g(x) \dd{x} = f(b)g(b) - f(a)g(a)  
    \]
    which completes the proof.
\end{prf}

\section{Change of Variable in a Riemann Integral}

\begin{thm}\label{thm4:feb14}
    Let $u \in \mathcal{D}[a,b]$, and $u' \in \mR[a,b]$ and suppose $f \in \mathcal{C}[u([a,b])]$ then 
    \begin{equation}\label{change_of_var}
        \int_a^b f\left(u(x)\right) u'(x) \dd{x} = \int_{u(a)}^{u(b)} f(x) \dd{x}    
    \end{equation}
\end{thm}

\begin{prf}
    First of all note that if $u$ is a constant function, then the theorem holds trivially. So we assume that $u$ is not a constant function. 

    Now since $f \in \mathcal{C}[u([a,b])]$ (note that $u([a,b])$ is a closed interval, since $u$ is a continuous function, hence image of $[a,b]$ under $u$ is also a closed interval), so we have $f \circ u \in \mR[a,b]$. Also since $u' \in \mR[a,b]$, we get that $(f \circ u) \cdot u' \in \mR[a,b]$, hence $\int_a^b f (u(x))u'(x) \dd{x}$ is well-defined.

    Now $\forall \, x \in u([a,b])$ we define 
    \[
        F(x) := \int_{u(a)}^x f(t) \dd{t}  
    \] 
    then by \texty{second fundamental theorem of calculus} $\ref{thm2:feb14}$, we get $F'(x) = f(x), \ \forall \, x \in [a,b]$.

    Also observe that from \texty{chain rule} we get that 
    \[
        \left( F \circ u \right)'(t) = F'(u(t))u'(t) = f(u(t))u'(t)  
    \]
    and finally we get that 
    \begin{align*}
        \int_a^b f(u(x))u'(x) \dd{x} &= \int_a^b \left( F \circ u \right)'(x) \dd{x} \\ 
        &\overset{(1)}{=} F(u(b)) - F(u(a)) \\ 
        &\overset{(2)}{=} \int_{u(a)}^{u(b)} f(x) \dd{x}
    \end{align*}
    where $(1)$ follows from \texty{first fundamental theorem of calculus} $\ref{thm1:feb14}$, and $(2)$ follows from the fact that $F(u(a)) = 0$.
\end{prf}
